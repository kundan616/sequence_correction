{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dna_sequence_correction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TirNQ6L-9gzD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "e4ea0555-1019-4606-9f0b-91d37dcbb414"
      },
      "source": [
        "!wget http://utrdb.ba.itb.cnr.it/utrdb-data/Databases/UTR/data/RefSeq70/3UTRef.Mam.fasta.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-24 08:31:10--  http://utrdb.ba.itb.cnr.it/utrdb-data/Databases/UTR/data/RefSeq70/3UTRef.Mam.fasta.gz\n",
            "Resolving utrdb.ba.itb.cnr.it (utrdb.ba.itb.cnr.it)... 150.145.82.18\n",
            "Connecting to utrdb.ba.itb.cnr.it (utrdb.ba.itb.cnr.it)|150.145.82.18|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 215598607 (206M) [application/x-gzip]\n",
            "Saving to: ‘3UTRef.Mam.fasta.gz’\n",
            "\n",
            "3UTRef.Mam.fasta.gz 100%[===================>] 205.61M  16.9MB/s    in 18s     \n",
            "\n",
            "2020-08-24 08:31:31 (11.5 MB/s) - ‘3UTRef.Mam.fasta.gz’ saved [215598607/215598607]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_xUWULP99Tj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gunzip /content/3UTRef.Mam.fasta.gz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpKVu91ZTwbF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "1c469c93-2ef5-47e3-f2e5-94f0b00e28a7"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "\n",
        "#reading and splitting the data\n",
        "print('loading the data................................') \n",
        "with open('3UTRef.Mam.fasta') as f:\n",
        "  j=f.read().split('>')\n",
        "print('batching the data................................') \n",
        "#taking a batch in consideration , for RAM issues\n",
        "j=j[0:100000]  \n",
        "\n",
        "# function for converting text to numbers a=0 c=1 g=2 t=3\n",
        "\n",
        "def oh(j):\n",
        "  x=j.replace('\\n','')[26:]\n",
        "  if set(x) == {'a', 'c', 'g', 't'}:\n",
        "    d=x.replace('a','1').replace('c','2').replace('g','3').replace('t','4')\n",
        "    a = np.array(list(map(int, d)))\n",
        "    #b = np.zeros((a.size, a.max()+1))\n",
        "    #b[np.arange(a.size),a] = 1\n",
        "    return a\n",
        "\n",
        "#running the above functiion and seeing a sample and making three arrays for x and y\n",
        "print('converting letter to number................................') \n",
        "data = list(map(oh, j[1:100000]))\n",
        "mod_data = list(map(oh, j[1:100000]))\n",
        "mod_data2 = list(map(oh, j[1:100000]))\n",
        "\n",
        "#removing None data present from first array\n",
        "a=1\n",
        "\n",
        "for j in range(60):\n",
        "  for i,d in enumerate(data):\n",
        "    if data[i] is None:\n",
        "      del data[i]\n",
        "      a+=1\n",
        "      #print(type(data[i]))\n",
        " \n",
        "\n",
        "#removing None data present from second array\n",
        "\n",
        "a=1\n",
        "\n",
        "for j in range(60):\n",
        "  for i,d in enumerate(mod_data):\n",
        "    if mod_data[i] is None:\n",
        "      del mod_data[i]\n",
        "      a+=1\n",
        "      #print(type(mod_data[i]))\n",
        "\n",
        "\n",
        "a=1\n",
        "\n",
        "for j in range(60):\n",
        "  for i,d in enumerate(mod_data2):\n",
        "    if mod_data2[i] is None:\n",
        "      del mod_data2[i]\n",
        "      a+=1\n",
        "      #print(type(mod_data2[i]))\n",
        "\n",
        "\n",
        "\n",
        "#randomly changing 10% characters from each example to make input data\n",
        "print('modifying the data................................') \n",
        "for i in range(len(mod_data)):\n",
        "#for i in range(1):  \n",
        "  l=int(len(mod_data[i])/2)\n",
        "  #print(l)\n",
        "  #for j in range(1):\n",
        "  for j in range(l):\n",
        "\n",
        "    mod_data[i][random.randint(0,len(mod_data[i])-1)]=random.choice([1,2,3,4])\n",
        "\n",
        "#checking if the modification worked\n",
        "index=3676\n",
        "fl=False\n",
        "for i in range(len(data[index])):\n",
        "  if data[index][i]!=mod_data[index][i]:\n",
        "    fl=True\n",
        "if fl:\n",
        "  print('data successfully modified')\n",
        "\n",
        "for i in range(len(data)):\n",
        "  if len(data[i])>3000:\n",
        "    data[i]=data[i][0:3000]\n",
        "\n",
        "for i in range(len(mod_data)):\n",
        "  if len(mod_data[i])>3000:\n",
        "    mod_data[i]=mod_data[i][0:3000]  \n",
        "\n",
        "for i in range(len(mod_data2)):\n",
        "  if len(mod_data2[i])>3000:\n",
        "    mod_data2[i]=mod_data2[i][0:3000] \n",
        "\n",
        "\n",
        "#zero padding\n",
        "print('padding................................') \n",
        "for i in range(len(data)):\n",
        "  if len(data[i])<3000:\n",
        "    s = [0] * (3000 - len(data[i]))\n",
        "    #print(type(data[i]))\n",
        "    data[i]=list(data[i])+s\n",
        "\n",
        "\n",
        "for i in range(len(mod_data2)):\n",
        "  if len(mod_data2[i])<3000:\n",
        "    s = [0] * (3000 - len(mod_data2[i]))\n",
        "    #print(type(data[i]))\n",
        "    mod_data2[i]=list(mod_data[i])+s\n",
        "\n",
        "\n",
        "for i in range(len(mod_data)):\n",
        "  if len(mod_data[i])<3000:\n",
        "    s = [0] * (3000 - len(mod_data[i]))\n",
        "    #print(type(data[i]))\n",
        "    mod_data[i]=s+list(mod_data[i])\n",
        "\n",
        "#one hot function\n",
        "def one_hot_con(data):\n",
        "  a = np.array(data)\n",
        "  b = np.zeros((a.size, 5))\n",
        "  b[np.arange(a.size),a] = 1\n",
        "  return b\n",
        "\n",
        "#making final processed dataset after converting one hot\n",
        "print('preparing the final data................................') \n",
        "X1 =np.array(list(map(one_hot_con, mod_data)))\n",
        "X2 =np.array(list(map(one_hot_con, mod_data2)))\n",
        "Y = np.array(list(map(one_hot_con, data)))\n",
        "\n",
        "\n",
        "print('pickeling................................') \n",
        "with open('X1.data', 'wb') as fh:\n",
        "   pickle.dump(X1, fh)\n",
        "\n",
        "with open('X2.data', 'wb') as fh:\n",
        "   pickle.dump(X2, fh)\n",
        "\n",
        "with open('Y.data', 'wb') as fh:\n",
        "   pickle.dump(Y, fh)  \n",
        "\n",
        "print('Successfully processed')       "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading the data................................\n",
            "batching the data................................\n",
            "converting letter to number................................\n",
            "modifying the data................................\n",
            "data successfully modified\n",
            "padding................................\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27oFR5FJMYcz",
        "colab_type": "text"
      },
      "source": [
        "MODEL CREATION , TRAINING , AND SAVING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9v4gUYC_ArOD",
        "colab": {}
      },
      "source": [
        "from random import randint\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import array_equal\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow import keras\n",
        "import pickle\n",
        "\n",
        "\n",
        "# returns train, inference_encoder and inference_decoder models\n",
        "def define_models():\n",
        "\t# define training encoder\n",
        "\tencoder_inputs = Input(shape=(3000,5))\n",
        "\tencoder = LSTM(512, return_state=True)\n",
        "\tencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\tencoder_states = [state_h, state_c]\n",
        "\t# define training decoder\n",
        "\tdecoder_inputs = Input(shape=(3000,5))\n",
        "\tdecoder_lstm = LSTM(512, return_sequences=True, return_state=True)\n",
        "\tdecoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "\tdecoder_dense = Dense(5, activation='softmax')\n",
        "\tdecoder_outputs = decoder_dense(decoder_outputs)\n",
        "\tmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\t# define inference encoder\n",
        "\tencoder_model = Model(encoder_inputs, encoder_states)\n",
        "\t# define inference decoder\n",
        "\tdecoder_state_input_h = Input(shape=(512,))\n",
        "\tdecoder_state_input_c = Input(shape=(512,))\n",
        "\tdecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\tdecoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "\tdecoder_states = [state_h, state_c]\n",
        "\tdecoder_outputs = decoder_dense(decoder_outputs)\n",
        "\tdecoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\t# return all models\n",
        "\treturn model, encoder_model, decoder_model\n",
        "\n",
        "#for loading the pickle in case of training later\n",
        "\n",
        "'''\n",
        "print('loading X1..........')\n",
        "pickle_x1 = open (\"X1.data\", \"rb\")\n",
        "X1 = pickle.load(pickle_x1)\n",
        "print('loading X2..........')\n",
        "pickle_x2 = open (\"X2.data\", \"rb\")\n",
        "X2 = pickle.load(pickle_x2)\n",
        "print('loading Y..........')\n",
        "pickle_y = open (\"Y.data\", \"rb\")\n",
        "Y = pickle.load(pickle_y)\n",
        "'''\n",
        "\n",
        "\n",
        "print('splitting the data.................')\n",
        "x1_train=X1[0:90000]\n",
        "x2_train=X2[0:90000]\n",
        "y_train=Y[0:90000]\n",
        "x1_test=X1[90000:]\n",
        "x2_test=X2[90000:]\n",
        "y_test=Y[90000:]\n",
        "\n",
        "\n",
        "print('creating model..........')\n",
        "model, encoder_model, decoder_model=define_models()\n",
        "model.summary()\n",
        "print('compiling model..........')\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0001,clipnorm=1.0)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt,\t\tmetrics=['accuracy'])\n",
        "print('starting the training..........')\n",
        "model.fit(\n",
        "    x=[x1_train,x2_train],\n",
        "    y=y_train,\n",
        "    batch_size=32,\n",
        "\t\tvalidation_data=([x1_test,x2_test],y_test),\n",
        "    epochs=100,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgVNdW6PL94T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('model.h5')\n",
        "encoder_model.save('encoder.h5')\n",
        "decoder_model.save('decoder.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFB8d3d92maW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "15d63fba-7aee-4d4d-98ac-3309b0d15190"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "print('loading model.........................')\n",
        "model = keras.models.load_model('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading model.........................\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVMr1teD42jK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#sequence length \n",
        "seqlen=3000\n",
        "\n",
        "#insert your sequence here\n",
        "seq='aaaaaaaa'\n",
        "\n",
        "print('replacing letters with numbers.........................')\n",
        "if set(seq) == {'a', 'c', 'g', 't'}:\n",
        "  d=seq.replace('a','1').replace('c','2').replace('g','3').replace('t','4')\n",
        "  a1 = np.array(list(map(int, d)))\n",
        "  a2 = np.array(list(map(int, d)))\n",
        "  print(a1)\n",
        "\n",
        "\n",
        "print('zero padding.........................')\n",
        "if len(a1)<seqlen:\n",
        "  s = [0] * (seqlen - len(a1))\n",
        "  a1=s+list(a1)\n",
        "#print(a1)\n",
        "\n",
        "\n",
        "if len(a2)<seqlen:\n",
        "  s = [0] * (seqlen - len(a2))\n",
        "  a2=list(a2)+s\n",
        "#print(a2)\n",
        "\n",
        "print('converting to one hot.........................')\n",
        "def one_hot_con(data):\n",
        "  a = np.array(data)\n",
        "  b = np.zeros((a.size, 5))\n",
        "  b[np.arange(a.size),a] = 1\n",
        "  return b\n",
        "\n",
        "x1=one_hot_con(a1)\n",
        "#print(x1)\n",
        "\n",
        "x2=one_hot_con(a2)\n",
        "#print(x2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('calculating output.........................')\n",
        "out_r=model.predict([x1.reshape(1,seqlen,5),x2.reshape(1,seqlen,5)])\n",
        "out_l=[]\n",
        "out=[]\n",
        "for i in range(len(seq)):\n",
        "  out.append(out_r[0][i])\n",
        "  out_l.append(list(out_r[0][i]))\n",
        "\n",
        "print('\\n')\n",
        "print('\\n')\n",
        "print('probablity distribution')\n",
        "print(out_l)\n",
        "print('first number in each array denotes the probablity of the network being not sure')\n",
        "\n",
        "print('\\n')\n",
        "print('\\n')\n",
        "print('\\n')\n",
        "ls=list(np.array(out).argmax(axis=1))\n",
        "st=''\n",
        "for i in ls:\n",
        "  st+=str(i)\n",
        "print('corrected sequence')\n",
        "print(seq)\n",
        "print(st.replace('0','x').replace('1','a').replace('2','c').replace('3','g').replace('4','t'))\n",
        "\n",
        "\n",
        "print('x : not sure')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}